#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥è‚¡ç¥¨è¡Œæƒ…æ”¶é›†è„šæœ¬
æ”¶é›†ç¾è‚¡å’Œ A è‚¡è¡Œæƒ…æ•°æ®ï¼Œç”Ÿæˆ Markdown æŠ¥å‘Š
"""

import json
import subprocess
import sys
import urllib.request
from datetime import datetime, timedelta
from pathlib import Path

# é…ç½®
WORKSPACE = Path(__file__).parent
DATA_DIR = WORKSPACE / "data"
REPORTS_DIR = WORKSPACE / "reports"
DATA_DIR.mkdir(exist_ok=True)
REPORTS_DIR.mkdir(exist_ok=True)

# Tavily API é…ç½®
TAVILY_API_KEY = "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"


def run_shell(cmd: str) -> str:
    """æ‰§è¡Œ shell å‘½ä»¤"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
        return result.stdout.strip()
    except Exception as e:
        return f"Error: {e}"


def get_tavily_news(query: str, max_results: int = 5) -> list:
    """ä» Tavily è·å–ç›¸å…³æ–°é—»"""
    try:
        url = "https://api.tavily.com/search"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {TAVILY_API_KEY}"
        }
        data = {
            "query": query,
            "max_results": max_results,
            "search_depth": "advanced",
            "include_answer": False
        }
        
        req = urllib.request.Request(url, data=json.dumps(data).encode(), headers=headers)
        with urllib.request.urlopen(req, timeout=30) as response:
            result = json.loads(response.read().decode())
        
        news_list = []
        for item in result.get("results", [])[:max_results]:
            news_list.append({
                "title": item.get("title", ""),
                "url": item.get("url", ""),
                "content": item.get("content", "")[:200] + "..." if len(item.get("content", "")) > 200 else item.get("content", "")
            })
        
        return news_list
    except Exception as e:
        return [{"title": f"è·å–å¤±è´¥ï¼š{str(e)}", "url": "", "content": ""}]


def get_us_stocks() -> dict:
    """è·å–ç¾è‚¡æ•°æ®ï¼ˆStooq APIï¼‰"""
    stocks = {}
    
    # ä¸‰å¤§æŒ‡æ•°
    indices = {
        "^DJI": "é“ç¼æ–¯",
        "^SPX": "æ ‡æ™® 500",
        "^IXIC": "çº³æ–¯è¾¾å…‹"
    }
    
    for symbol, name in indices.items():
        # Get two lines (header + data), take the second line
        data = run_shell(f'curl -sL "https://stooq.com/q/l/?s={symbol}&f=sd2t2ohlcv&h=x&t=x" 2>/dev/null | tail -1')
        if data and "N/D" not in data and data.strip():
            parts = data.split(",")
            # Format: Symbol,Date,Time,Open,High,Low,Close,Volume
            if len(parts) >= 7:
                stocks[name] = {
                    "symbol": symbol,
                    "date": parts[1],
                    "close": parts[6],
                    "open": parts[3],
                    "high": parts[4],
                    "low": parts[5],
                    "volume": parts[7] if len(parts) > 7 else "-"
                }
    
    # ä¸ƒå§å¦¹ä¸ªè‚¡
    tech_stocks = {
        "AAPL.US": "è‹¹æœ",
        "MSFT.US": "å¾®è½¯",
        "GOOGL.US": "è°·æ­Œ",
        "AMZN.US": "äºšé©¬é€Š",
        "NVDA.US": "è‹±ä¼Ÿè¾¾",
        "META.US": "Meta",
        "TSLA.US": "ç‰¹æ–¯æ‹‰"
    }
    
    for symbol, name in tech_stocks.items():
        # Get two lines (header + data), take the second line
        data = run_shell(f'curl -sL "https://stooq.com/q/l/?s={symbol}&f=sd2t2ohlcv&h=x&t=x" 2>/dev/null | tail -1')
        if data and "N/D" not in data and data.strip():
            parts = data.split(",")
            # Format: Symbol,Date,Time,Open,High,Low,Close,Volume
            if len(parts) >= 7:
                stocks[name] = {
                    "symbol": symbol.replace(".US", ""),
                    "date": parts[1],
                    "close": parts[6],
                    "open": parts[3],
                    "high": parts[4],
                    "low": parts[5],
                    "volume": parts[7] if len(parts) > 7 else "-"
                }
    
    return stocks


def get_cn_stocks() -> dict:
    """è·å– A è‚¡æ•°æ®ï¼ˆæ–°æµªè´¢ç» APIï¼‰"""
    stocks = {}
    
    # ä¸‰å¤§æŒ‡æ•°
    indices = {
        "sh000001": "ä¸Šè¯æŒ‡æ•°",
        "sz399001": "æ·±è¯æˆæŒ‡",
        "cyb": "åˆ›ä¸šæ¿æŒ‡"
    }
    
    for code, name in indices.items():
        data = run_shell(f'curl -sL "http://hq.sinajs.cn/list={code}" 2>/dev/null')
        if data and code in data:
            try:
                # è§£ææ•°æ®ï¼švar hq_str_sh000001="åç§°ï¼Œå½“å‰ï¼Œæ˜¨æ—¥ï¼Œä»Šå¤©ï¼Œæœ€é«˜ï¼Œæœ€ä½ï¼Œ..."
                content = data.split('"')[1] if '"' in data else ""
                parts = content.split(",")
                if len(parts) >= 5:
                    stocks[name] = {
                        "symbol": code,
                        "close": parts[3] if parts[3] else "-",
                        "open": parts[1] if parts[1] else "-",
                        "high": parts[4] if parts[4] else "-",
                        "low": parts[5] if parts[5] else "-",
                        "change": calc_change(parts[3], parts[2]) if len(parts) > 2 and parts[2] else "-"
                    }
            except Exception as e:
                stocks[name] = {"error": str(e)}
    
    return stocks


def get_market_news() -> dict:
    """ä» Tavily è·å–å¸‚åœºæ–°é—»"""
    news = {
        "us_news": [],
        "cn_news": [],
        "global_news": []
    }
    
    # ç¾è‚¡æ–°é—»
    us_query = "ç¾è‚¡å¸‚åœºæ–°é—» ç¾è”å‚¨ ç§‘æŠ€è‚¡ 2026 å¹´ 2 æœˆ"
    news["us_news"] = get_tavily_news(us_query, max_results=3)
    
    # A è‚¡æ–°é—»
    cn_query = "A è‚¡å¸‚åœºæ–°é—» ä¸­å›½è¯ç›‘ä¼š æ²ªæ·±è‚¡å¸‚ 2026 å¹´ 2 æœˆ"
    news["cn_news"] = get_tavily_news(cn_query, max_results=3)
    
    # å…¨çƒè´¢ç»æ–°é—»
    global_query = "å…¨çƒè´¢ç»æ–°é—» äººå·¥æ™ºèƒ½ AI èŠ¯ç‰‡ åŠ å¯†è´§å¸ 2026 å¹´"
    news["global_news"] = get_tavily_news(global_query, max_results=3)
    
    return news


def calc_change(current: str, prev: str) -> str:
    """è®¡ç®—æ¶¨è·Œå¹…"""
    try:
        curr = float(current)
        prev = float(prev)
        if prev == 0:
            return "0%"
        change = ((curr - prev) / prev) * 100
        return f"{change:+.2f}%"
    except:
        return "-"


def generate_report(date: str, us_data: dict, cn_data: dict, news_data: dict = None) -> str:
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    report = f"""# ğŸ“ˆ æ¯æ—¥è‚¡ç¥¨è¡Œæƒ… | {date}

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")} (Asia/Shanghai)

---

## ğŸ‡ºğŸ‡¸ ç¾è‚¡å¸‚åœº

### ä¸‰å¤§æŒ‡æ•°

| æŒ‡æ•° | ä»£ç  | æ”¶ç›˜ä»· | å¼€ç›˜ | æœ€é«˜ | æœ€ä½ | æˆäº¤é‡ |
|------|------|--------|------|------|------|--------|
"""
    
    us_indices = ["é“ç¼æ–¯", "æ ‡æ™® 500", "çº³æ–¯è¾¾å…‹"]
    for name in us_indices:
        if name in us_data:
            d = us_data[name]
            report += f"| {name} | {d['symbol']} | {d['close']} | {d['open']} | {d['high']} | {d['low']} | {d['volume']} |\n"
    
    report += """
### ç§‘æŠ€ä¸ƒå§å¦¹

| å…¬å¸ | ä»£ç  | æ”¶ç›˜ä»· | å¼€ç›˜ | æœ€é«˜ | æœ€ä½ |
|------|------|--------|------|------|------|
"""
    
    tech = ["è‹¹æœ", "å¾®è½¯", "è°·æ­Œ", "äºšé©¬é€Š", "è‹±ä¼Ÿè¾¾", "Meta", "ç‰¹æ–¯æ‹‰"]
    for name in tech:
        if name in us_data:
            d = us_data[name]
            report += f"| {name} | {d['symbol']} | ${d['close']} | ${d['open']} | ${d['high']} | ${d['low']} |\n"
    
    report += f"""
---

## ğŸ‡¨ğŸ‡³ A è‚¡å¸‚åœº

### ä¸‰å¤§æŒ‡æ•°

| æŒ‡æ•° | ä»£ç  | æ”¶ç›˜ä»· | å¼€ç›˜ | æœ€é«˜ | æœ€ä½ | æ¶¨è·Œå¹… |
|------|------|--------|------|------|------|--------|
"""
    
    cn_indices = ["ä¸Šè¯æŒ‡æ•°", "æ·±è¯æˆæŒ‡", "åˆ›ä¸šæ¿æŒ‡"]
    for name in cn_indices:
        if name in cn_data:
            d = cn_data[name]
            report += f"| {name} | {d.get('symbol', '-')} | {d.get('close', '-')} | {d.get('open', '-')} | {d.get('high', '-')} | {d.get('low', '-')} | {d.get('change', '-')} |\n"
    
    report += f"""
---

## ğŸ“° å¸‚åœºæ–°é—» (Tavily)

"""
    
    if news_data:
        # ç¾è‚¡æ–°é—»
        if news_data.get("us_news"):
            report += "### ğŸ‡ºğŸ‡¸ ç¾è‚¡æ–°é—»\n\n"
            for news in news_data["us_news"]:
                if news.get("title") and "è·å–å¤±è´¥" not in news.get("title", ""):
                    report += f"- **{news['title']}**\n"
                    if news.get("url"):
                        report += f"  ğŸ”— [{news['url']}]({news['url']})\n"
                    if news.get("content"):
                        report += f"  {news['content']}\n"
                    report += "\n"
        
        # A è‚¡æ–°é—»
        if news_data.get("cn_news"):
            report += "### ğŸ‡¨ğŸ‡³ A è‚¡æ–°é—»\n\n"
            for news in news_data["cn_news"]:
                if news.get("title") and "è·å–å¤±è´¥" not in news.get("title", ""):
                    report += f"- **{news['title']}**\n"
                    if news.get("url"):
                        report += f"  ğŸ”— [{news['url']}]({news['url']})\n"
                    if news.get("content"):
                        report += f"  {news['content']}\n"
                    report += "\n"
        
        # å…¨çƒè´¢ç»æ–°é—»
        if news_data.get("global_news"):
            report += "### ğŸŒ å…¨çƒè´¢ç»\n\n"
            for news in news_data["global_news"]:
                if news.get("title") and "è·å–å¤±è´¥" not in news.get("title", ""):
                    report += f"- **{news['title']}**\n"
                    if news.get("url"):
                        report += f"  ğŸ”— [{news['url']}]({news['url']})\n"
                    if news.get("content"):
                        report += f"  {news['content']}\n"
                    report += "\n"
    else:
        report += "*ä»Šæ—¥æ–°é—»æ•°æ®æš‚ç¼º*\n\n"
    
    report += f"""---

## ğŸ“Š æ•°æ®æº

| ç±»å‹ | æ•°æ®æº | è¯´æ˜ |
|------|--------|------|
| ç¾è‚¡è¡Œæƒ… | Stooq | å…¬å¼€å…è´¹ APIï¼Œå»¶è¿Ÿ 15-20 åˆ†é’Ÿ |
| A è‚¡è¡Œæƒ… | æ–°æµªè´¢ç» | å…¬å¼€å…è´¹ APIï¼Œå®æ—¶ |
| å¸‚åœºæ–°é—» | Tavily | AI æœç´¢å¼•æ“ï¼Œå®æ—¶èµ„è®¯ |

---

*ğŸ¤– è‡ªåŠ¨ç”Ÿæˆ | æ•°æ®ä»…ä¾›å‚è€ƒ*
"""
    
    return report


def save_data(date: str, us_data: dict, cn_data: dict, news_data: dict = None):
    """ä¿å­˜åŸå§‹æ•°æ®åˆ° JSON"""
    data = {
        "date": date,
        "timestamp": datetime.now().isoformat(),
        "us_stocks": us_data,
        "cn_stocks": cn_data,
        "news": news_data
    }
    
    with open(DATA_DIR / f"{date}.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    date = datetime.now().strftime("%Y-%m-%d")
    weekday = datetime.now().strftime("%A")
    
    print(f"ğŸ“Š å¼€å§‹æ”¶é›† {date} ({weekday}) çš„è‚¡ç¥¨æ•°æ®...")
    
    # æ”¶é›†æ•°æ®
    print("ğŸ‡ºğŸ‡¸ æ”¶é›†ç¾è‚¡æ•°æ®...")
    us_data = get_us_stocks()
    print(f"   æ”¶é›†åˆ° {len(us_data)} æ¡ç¾è‚¡æ•°æ®")
    
    print("ğŸ‡¨ğŸ‡³ æ”¶é›† A è‚¡æ•°æ®...")
    cn_data = get_cn_stocks()
    print(f"   æ”¶é›†åˆ° {len(cn_data)} æ¡ A è‚¡æ•°æ®")
    
    print("ğŸ“° æ”¶é›†å¸‚åœºæ–°é—» (Tavily)...")
    news_data = get_market_news()
    print(f"   æ”¶é›†åˆ° {len(news_data.get('us_news', []))} æ¡ç¾è‚¡æ–°é—»ï¼Œ{len(news_data.get('cn_news', []))} æ¡ A è‚¡æ–°é—»")
    
    # ä¿å­˜æ•°æ®
    save_data(date, us_data, cn_data, news_data)
    print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {DATA_DIR}/{date}.json")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(date, us_data, cn_data, news_data)
    report_path = REPORTS_DIR / f"{date}.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"ğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_path}")
    
    # æ›´æ–° README
    update_readme(date, us_data, cn_data)
    
    print("âœ… å®Œæˆï¼")
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“Š ä»Šæ—¥æ‘˜è¦:")
    if "é“ç¼æ–¯" in us_data:
        print(f"   é“ç¼æ–¯ï¼š{us_data['é“ç¼æ–¯']['close']}")
    if "ä¸Šè¯æŒ‡æ•°" in cn_data:
        print(f"   ä¸Šè¯æŒ‡æ•°ï¼š{cn_data['ä¸Šè¯æŒ‡æ•°'].get('close', '-')}")


def update_readme(date: str, us_data: dict, cn_data: dict):
    """æ›´æ–° README.md"""
    readme_path = WORKSPACE / "README.md"
    
    # è¯»å–ç°æœ‰ README æˆ–åˆ›å»ºæ–°çš„
    if readme_path.exists():
        with open(readme_path, "r", encoding="utf-8") as f:
            content = f.read()
    else:
        content = """# ğŸ“ˆ æ¯æ—¥è‚¡ç¥¨è¡Œæƒ…è·Ÿè¸ª

è‡ªåŠ¨æ”¶é›†ç¾è‚¡å’Œ A è‚¡æ¯æ—¥è¡Œæƒ…æ•°æ®ï¼Œç”Ÿæˆ Markdown æŠ¥å‘Šã€‚

## æ•°æ®æº

- **ç¾è‚¡**: Stooq (https://stooq.com)
- **A è‚¡**: æ–°æµªè´¢ç» (http://hq.sinajs.cn)

## ç›®å½•ç»“æ„

```
stock-daily/
â”œâ”€â”€ data/          # åŸå§‹ JSON æ•°æ®
â”œâ”€â”€ reports/       # Markdown æŠ¥å‘Š
â”œâ”€â”€ collect_stocks.py  # æ•°æ®æ”¶é›†è„šæœ¬
â””â”€â”€ README.md      # æœ¬æ–‡ä»¶
```

## å®šæ—¶ä»»åŠ¡

æ¯å¤© 08:30 (Asia/Shanghai) è‡ªåŠ¨è¿è¡Œï¼Œæ›´æ–°æ•°æ®å¹¶æ¨é€åˆ° GitHubã€‚

## æœ€æ–°è¡Œæƒ…

"""
    
    # ç”Ÿæˆæœ€æ–°è¡Œæƒ…æ‘˜è¦
    latest = f"""### {date}

**ç¾è‚¡**: """
    if "é“ç¼æ–¯" in us_data:
        latest += f"é“ç¼æ–¯ {us_data['é“ç¼æ–¯']['close']} | "
    if "æ ‡æ™® 500" in us_data:
        latest += f"æ ‡æ™® {us_data['æ ‡æ™® 500']['close']} | "
    if "çº³æ–¯è¾¾å…‹" in us_data:
        latest += f"çº³æŒ‡ {us_data['çº³æ–¯è¾¾å…‹']['close']}"
    
    latest += "\n\n**A è‚¡**: "
    if "ä¸Šè¯æŒ‡æ•°" in cn_data:
        latest += f"ä¸Šè¯ {cn_data['ä¸Šè¯æŒ‡æ•°'].get('close', '-')} | "
    if "æ·±è¯æˆæŒ‡" in cn_data:
        latest += f"æ·±è¯ {cn_data['æ·±è¯æˆæŒ‡'].get('close', '-')} | "
    if "åˆ›ä¸šæ¿æŒ‡" in cn_data:
        latest += f"åˆ›ä¸šæ¿ {cn_data['åˆ›ä¸šæ¿æŒ‡'].get('close', '-')}"
    
    latest += "\n\n---\n"
    
    # æ’å…¥åˆ°"## æœ€æ–°è¡Œæƒ…"åé¢
    if "## æœ€æ–°è¡Œæƒ…" in content:
        content = content.replace("## æœ€æ–°è¡Œæƒ…\n", f"## æœ€æ–°è¡Œæƒ…\n{latest}")
    else:
        content += f"\n## æœ€æ–°è¡Œæƒ…\n{latest}"
    
    with open(readme_path, "w", encoding="utf-8") as f:
        f.write(content)
    
    print(f"ğŸ“ README å·²æ›´æ–°")


if __name__ == "__main__":
    main()
